# Concepts and Relations Synthesis

We propose a knowledge graph approach to synthesize concepts and relations that are studied in social science publications on the topic of social media. We collected Social Sciences Citation Index full-text publications (until the end of 2019) from Web of Science and demonstrate how to synthesize concepts and relations from these publications using our method. We present the scripts of the extraction process and parts of the synthesized data.

## Scripts
The scripts can be divided into two parts. (1) The first part processes the publication data (e.g., preprocess the original text data, transform the manually coded data into training data for model training, and process the automatically coded data). This part of scripts are in the folder "Data Processing." (2) The second part trains seven deep learning models using the manually coded data and employs the trained models to automatically code the text data. This part of scripts are in the folder "Model Training and Automatic Extraction." It should be noted that the second part of scripts should be run on Google Colab (which is a free online platform for anybody to run python code through the browser and is especially well suited to machine learning). When you execute the second part of scripts on Google Colab, remember to upload the corresponding input data to Google Colab. The input data are provided in the folder "Data."

## Data
All data (including original, middle, and final data) are provided in the folder "Data." We employ 249 manually coded publications to train models and extract the concepts and relations from 12,563 publications automatically in our study. Here we also employ 249 manually coded publications to train models but only extract the concepts and relations from 1,324 publications (around 10% of 12,563 publications).

## Brief Introduction of the Pipeline
**Preprocessing:** Users can first run "Data Processing/0_PdfText_to_HypoList.ipynb" to process txt files of publications with hypothesis/result statements (We provide ten example txt files in "Data/Pdf Texts") into structured data in a JSON file "Data/data.json". The scripts use regular expressions to determine which sentences in publications are possibly hypothesis/result statements.

**Manual Extraction:** Users then select a sample of publications and manually encode the required information (the detailed process of manual coding is introduced in section "Manually Code A Training Dataset with High Quality" in our paper) of these publications based on their extracted sentences in "Data/data.json" and record the manually coded data into a JSON file "Data/data3.json." In "Data/data3.json", we provide the data of 1,573 publications. 249 publications have manually coded information and the other 1,324 publications only have sentences that are possibly hypothesis/result statements automatically extracted in Preprocessing.

**Model Training and Automatic Extraction:** We use seven deep models to learn the manually coded information of the 249 publications in "Data/data3.json" and automatically code the other 1,324 publications. This process consists of seven steps as follows.

*Step 1.* Run Blocks 1 and 2 of "Data Processing/1_ExtractHyposen.ipynb" to prepare the training and prediction data, respectively. Run "Model Training and Automatic Extraction/1_ExtractHyposen_Model.ipynb" to train and automatically predict the labels. Run Block 3 of "Data Processing/1_ExtractHyposen.ipynb" to process the predicted labels.

*Steps 2-3.* Run Blocks 1 and 2 of "Data Processing/2_ExtractHypoVar_Role.ipynb" to prepare the training and prediction data for both Steps 2 and 3. Run "Model Training and Automatic Extraction/2.2_ExtractHypoVarRole_Model.ipynb" to train and automatically predict the labels in Step 2. Run "Model Training and Automatic Extraction/2.1_ExtractHypoPosneg_Model.ipynb" to train and automatically predict the labels in Step 3. Run Block 3 of "Data Processing/2_ExtractHypoVar_Role.ipynb" to process the predicted labels in Steps 2 and 3.

*Step 4.* Run Blocks 1 and 2 of "Data Processing/3_ExtractConceLabel.ipynb" to prepare the training and prediction data for both Steps 4. Run "Model Training and Automatic Extraction/3_ExtractConceLabel_Model.ipynb" to train and automatically predict the labels. Run Block 3 of "Data Processing/3_ExtractConceLabel.ipynb" to process the predicted labels.

*Step 5.* Run Blocks 1 and 2 of "Data Processing/4_ExtractResusen.ipynb"  to prepare the training and prediction data for both Steps 5. Run "Model Training and Automatic Extraction/4_ExtractResusen_Model.ipynb" to train and automatically predict the labels. 

*Steps 6-7.* Run Blocks 1 and 2 of "Data Processing/5_ExtractID_Support_Pos.ipynb" to prepare the training and prediction data for both Steps 6 and 7. Run "Model Training and Automatic Extraction/5.2_ExtractResu_Support_Model.ipynb" to train and automatically predict the labels in Step 6. Run "Model Training and Automatic Extraction/5.1_ExtractResu_Posneg_Model.ipynb" to train and automatically predict the labels in Step 7. Run Block 3 of "Data Processing/5_ExtractID_Support_Pos.ipynb" to process the predicted labels in Steps 6 and 7.

"Data Processing/6_MatchHypoResu.ipynb" matches the hypothesis and result statements into the final data "Data/data_whole.json."

We also provide a data table "Data/Synthesized_Concepts_and_Relations.xlsx" that includes the extracted results in our study. The table includes 10,312 hypothesis statements that deep learning models automatically extract from 2,255 publications on the topic of social media. Each hypothesis statement corresponds to a row. For each hypothesis statement, the hypothesis ID, variable terms, and concepts of the variable terms that deep learning models extract are presented. A pair of variable terms/concepts represent a relation. In our study, we analyze this data to obtain the insights of concepts and relations in social science publications on the topic of social media. It should be noted that the models extract four levels of concepts for the variable terms and we only present level-one concepts here. Besides, the models also extract properties of the relation in a hypothesis statement, such as whether the relation is empirically supported and its polarity.
